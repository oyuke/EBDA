import pytest
import pandas as pd
import os
from core.io import ConfigLoader
from core.quality import QualityGateway
from core.decision import DecisionEngine
from core.priority import PriorityCalculator

def test_full_workflow():
    # 1. Load Config
    loader = ConfigLoader("configs/customer_default.yaml")
    config = loader.load_config()
    assert len(config.decision_cards) > 0
    
    # 2. Load Sample Data (Generated by script)
    survey_path = "data/sample_survey.csv"
    kpi_path = "data/sample_kpi.csv"
    
    if not os.path.exists(survey_path):
        pytest.skip("Sample data not generated yet")
        
    df_survey = pd.read_csv(survey_path)
    df_kpi = pd.read_csv(kpi_path)
    
    # 3. Quality Gate
    gateway = QualityGateway(config.quality_gates)
    penalty, checks = gateway.check_survey_data(df_survey)
    # Expect some penalty due to missing data injected in generator
    assert penalty >= 0.0
    
    # 4. Prepare Context
    context = {}
    # Driver scores
    for driver in config.drivers:
        cols = [c for c in driver.survey_items if c in df_survey.columns]
        if cols:
            context[driver.id] = df_survey[cols].mean().mean()
            
    # KPI scores
    if "turnover_rate_junior" in df_kpi.columns:
        context["turnover_rate_junior"] = df_kpi["turnover_rate_junior"].iloc[-1]
    
    # 5. Decision Engine
    engine = DecisionEngine()
    
    # Check specific card D001 (Turnover)
    card_d001 = next(c for c in config.decision_cards if c.id == "D001")
    state = engine.evaluate_card(card_d001, context)
    
    # We expect RED or YELLOW depending on random data, but definitely not error
    assert state.status in ["RED", "YELLOW", "GREEN"]
    
    # 6. Priority
    calc = PriorityCalculator(config.priority_weights)
    p_res = calc.calculate_saw(impact=0.8, urgency=0.5, uncertainty=penalty)
    assert p_res["score"] > 0
